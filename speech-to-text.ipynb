from pyannote.audio import Pipeline
import whisper
from huggingface_hub import login

# Replace with your Hugging Face access token
HF_TOKEN = "hglkjhvcx"

# Load Pyannote diarization pipeline
print("Loading speaker diarization model...")
from huggingface_hub import login
login(HF_TOKEN)  # logs in using your Hugging Face token

diarization_pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization")


# Load Whisper model
print("Loading Whisper model...")
whisper_model = whisper.load_model("base")

# Path to your .wav file
AUDIO_FILE = "1.wav"

# Step 1: Perform speaker diarization
print("Performing speaker diarization...")
diarization_result = diarization_pipeline(AUDIO_FILE)

# Step 2: Perform transcription
print("Transcribing audio...")
transcription = whisper_model.transcribe(AUDIO_FILE)["segments"]

# Step 3: Match diarization timestamps with transcript
print("Combining results...")

speaker_segments = []
for turn, _, speaker in diarization_result.itertracks(yield_label=True):
    speaker_segments.append({
        "start": turn.start,
        "end": turn.end,
        "speaker": speaker
    })

def find_speaker_for_segment(start, end):
    for seg in speaker_segments:
        if seg["start"] <= start <= seg["end"]:
            return seg["speaker"]
    return "Unknown"

# Step 4: Create formatted conversation text
conversation_text = "==== Conversation ====\n\n"

for seg in transcription:
    start = seg["start"]
    end = seg["end"]
    text = seg["text"].strip()
    speaker_label = find_speaker_for_segment(start, end)

    if speaker_label == "SPEAKER_00":
        speaker_name = "Sales Representative"
    else:
        speaker_name = "Customer"

    line = f"{speaker_name}: {text}\n"
    print(line.strip())
    conversation_text += line

# Step 5: Save conversation to file
OUTPUT_FILE = "1.txt"
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    f.write(conversation_text)

print(f"\n Conversation saved successfully to '{OUTPUT_FILE}'!")

